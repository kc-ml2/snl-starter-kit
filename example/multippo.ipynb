{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "colab": {
      "name": "train.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "Zmt5bZ1N8zsF"
      },
      "source": [
        "# Decentralized N single PPO agents training\n",
        "\n",
        "본 노트북에서는 잘 알려진 알고리즘인 PPO알고리즘을 활용하여,\n",
        "\n",
        "competitive setting에서의 학습을 진행해보겠습니다.\n",
        "\n",
        "N개의 동일한 PPO agents와, centralized critic이 없는 상황에서 얼마나 똑똑한 뱀이 학습 될 수 있는지 확인해 보겠습니다.\n",
        "\n",
        "TODO:\n",
        "\n",
        "Some more desc.\n",
        "(이론(TODO: link to 이론)에서 설명드렸다시피, single agent가 multi agents상황에서 emergent behavior~)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RomUCOhH8zsK"
      },
      "source": [
        "ML2에서 자체 개발한 `rl2`라이브러리를 사용하여 학습 해보겠습니다.\n",
        "\n",
        "rl2의 자세한 사용법은 https://github.com/kc-ml2/rl2 를 참고 부탁드립니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHQxwgEPBKeZ"
      },
      "source": [
        "## 셋업"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EO2P_zsq9XgR"
      },
      "source": [
        "## 패키지 설치"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Tx7v_Hi9RcY"
      },
      "source": [
        "!pip3 intsall marlenv rl2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1E7IfRm9dq8"
      },
      "source": [
        "## 패키지 로드"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ojAZaGsl8zsL"
      },
      "source": [
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QkNcUpP58zsL"
      },
      "source": [
        "import sys\n",
        "from pprint import pprint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Kh-5UhO8zsL"
      },
      "source": [
        "### 학습 디바이스 세팅"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8a6V3mPC8zsM"
      },
      "source": [
        "import os\n",
        "\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkUNUU7R8zsM"
      },
      "source": [
        "## Env"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8jgD5nZh8zsM"
      },
      "source": [
        "게임 세팅에 적합한 initialization을 해주는 helper factory 메소드인 `make_snake`를 통해 env를 초기화 합니다.\n",
        "\n",
        "e.g. `n_env` > 1에 대하여 gym의 VecEnv와 동일한 개념의 vectorized env로 초기화가 됩니다.\n",
        "\n",
        "자세한 사용법은 https://github.com/kc-ml2/marlenv 를 참고 하세요!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jrGvH-7X8zsM"
      },
      "source": [
        "from marlenv.wrappers import make_snake"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3sdsFX5e8zsN"
      },
      "source": [
        "학습할 환경의 세팅은 아래와 같습니다.\n",
        "\n",
        "PPO는 vectorized env에서 학습이 잘 되는 것으로 알려 져있기 떄문에, `n_env=64`로 세팅합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKg6QJvf8zsN"
      },
      "source": [
        "env_config = dict(\n",
        "    num_snakes=3,\n",
        "    n_env=64,\n",
        "    height=20,\n",
        "    width=20,\n",
        "    frame_stack=2,\n",
        "    vision_range=5,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FoqySdtF8zsN"
      },
      "source": [
        "env, obs_shape, ac_shape, props = make_snake(**env_config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HrLsoqm8zsN"
      },
      "source": [
        "`props` 딕셔너리는 초기화 변수에 따른 환경 세팅의 특성들을 담고 있습니다.\n",
        "\n",
        "`reorder`는 tensorflow와 torch의 컨볼루션 레이어가 받는 shape의 순서가 다르기 때문에, 재배열을 해주는 파라미터입니다.\n",
        "\n",
        "gym 컨벤션을 따라서, default값이 False(tensorflow)입니다.\n",
        "\n",
        "p.s. `tensorflow`는 `(N, H, W, C)`, `torch`는 `(N, C, H, W)`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gA_8lWWl8zsO"
      },
      "source": [
        "pprint(list(props.keys()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igC9aU668zsO"
      },
      "source": [
        "## Config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-uasZ9b8zsO"
      },
      "source": [
        "config = dict(\n",
        "    batch_size=512, \n",
        "    epoch=4, \n",
        "    train_interval=128, \n",
        "    log_level=10,\n",
        "    log_interval=5e4, \n",
        "    save_interval=1e6, \n",
        "    lr=1e-4, \n",
        "    gamma=0.99,\n",
        "    grad_clip=10,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQ3hKLoMBW9s"
      },
      "source": [
        "# TODO: remove dependency on easydict\n",
        "from easydict import EasyDict\n",
        "config=EasyDict(config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSQpzdZd8zsP"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6Breggk8zsP"
      },
      "source": [
        "`rl2`의 predefined model인 `PPOModel` 클래스를 사용하여 학습하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2IRGjdd8zsP"
      },
      "source": [
        "from rl2.agents.ppo import PPOModel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JD0-jyFy8zsP"
      },
      "source": [
        "model = PPOModel(\n",
        "    obs_shape,\n",
        "    ac_shape,\n",
        "    recurrent=False,\n",
        "    discrete=True,\n",
        "    reorder=props['reorder'],\n",
        "    optimizer='torch.optim.RMSprop',\n",
        "    high=props['high']\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jTuBsVb8zsP"
      },
      "source": [
        "## Agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXb2DtoT8zsQ"
      },
      "source": [
        "`rl2`의 predefined model인 `PPOAgent` 클래스를 사용하여 학습하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3ujc0dG8zsQ"
      },
      "source": [
        "from rl2.agents.ppo import PPOAgent"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XuLGsekv8zsQ"
      },
      "source": [
        "def ppo_agent():\n",
        "    agent = PPOAgent(\n",
        "        model,\n",
        "        train_interval=config['train_interval'],\n",
        "        n_env=env_config['n_env'],\n",
        "        batch_size=config['batch_size'],\n",
        "        num_epochs=config['epoch'],\n",
        "        buffer_kwargs={\n",
        "            'size': config['train_interval'],\n",
        "            'n_env': env_config['n_env'],\n",
        "        }\n",
        "    )\n",
        "    \n",
        "    return agent"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYRw9JdD8zsQ"
      },
      "source": [
        "agents = [ppo_agent() for _ in range(env_config['num_snakes'])]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYpCSzRl8zsQ"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PayggQR88zsR"
      },
      "source": [
        "Agent와 Env의 interaction을 진행시켜주는 Worker 클래스를 사용하여 학습을 진행합니다.\n",
        "\n",
        "`MaxStepWorker`는 클래스명처럼, 주어진 step 횟수만큼 interaction(rollout)을 진행시키고 종료 합니다. 에피소드 단위로 진행하고 싶으시면 `EpisodicWorker` 메뉴얼을 참고 부탁드립니다.\n",
        "\n",
        "Single Agent일때 사용하는 `MaxStepWorker`의 MA버전인 `MAMaxStepWorker`를 사용합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UpcOUTXV8zsR"
      },
      "source": [
        "from rl2.workers.multi_agent import MAMaxStepWorker"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDkSsc_V_IWi"
      },
      "source": [
        "### Logger\n",
        "\n",
        "`rl2`에서는 본인이 원하는 custom logger 클래스를 정의하여 로깅을 할 수 있습니다.\n",
        "\n",
        "예시로 제공하는 `Logger` 클래스를 활용하여 로깅을 해보겠습니다.\n",
        "\n",
        "해당 클래스는 tensorboard의 `FileWriter`객체를 가지고 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hsdu5COa8zsR"
      },
      "source": [
        "from rl2.examples.temp_logger import Logger"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5JLr-SrAEfw"
      },
      "source": [
        "로깅 componenets\n",
        "* checkpoint\n",
        "* tensorboard summary data\n",
        "* env setting\n",
        "* etc"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j33gW3yR8zsR"
      },
      "source": [
        "config['log_dir'] = './MAPPO'\n",
        "logger = Logger(name='MAPPO', args=config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AzsOujPf8zsR"
      },
      "source": [
        "worker = MAMaxStepWorker(\n",
        "    env, \n",
        "    props['n_env'], \n",
        "    agents,\n",
        "    max_steps=int(1e3),\n",
        "    training=True,\n",
        "    logger=logger,\n",
        "    log_interval=config['log_interval'],\n",
        "    render=True,\n",
        "    render_interval=int(5e5),\n",
        "    is_save=True,\n",
        "    save_interval=config['save_interval'],\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "eVQlwCAM8zsS"
      },
      "source": [
        "worker.run()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwpxQwOU_OlN"
      },
      "source": [
        "## 텐서보드"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_nWLcOKJ8zsS"
      },
      "source": [
        "%tensorboard --logdir ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VzIhStbH9vdd"
      },
      "source": [
        "## 에이전트 검증"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INynbyFy9rex"
      },
      "source": [
        "### SavedModel 로드"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jkxc_vAsBhmF"
      },
      "source": [
        "### Sample Rollouts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0GTCUjlBnaY"
      },
      "source": [
        "### Docker Containerize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SoFKMksLBqTJ"
      },
      "source": [
        "### Validate Container"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PfX9S63i8zsS"
      },
      "source": [
        "## 에이전트 제출"
      ]
    }
  ]
}